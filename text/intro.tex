\section{Introduction}
\label{sec:intro}



% Heterogeneous compute units within a node have become a fact of life.  Over 60
% machines on the Top500 list in November of 2012 employ GPUs as accelerators.
% Computers in the consumer sector, from desktops to phones, are increasingly
% adopting them as well.  In contrast to their forebears, relying on increases in
% CPU performance or even number of CPU cores, these systems cannot be fully
% utilized with legacy code alone.  It is typical for CPU/GPU systems, for
% example, to derive $50\%$ or more of their peak FLOPS from accelerators. Despite
% this fact, many users simply continue to run legacy CPU codes on them.

% In response, progressively simpler and more familiar programming models have
% been created, easing the transition from shared memory thread programming.
% However, the
% nearly universal lack in these systems is a mechanism by which to employ
% \emph{more than one} address space.  For example, using ones CPU cores and a
% GPU, or multiple GPUs, is generally not possible without significant contortions
% in even the si
% to improve the programmability or performance of code on heterogeneous systems.
% Some accomplish this by creating entirely new programming paradymes

%largely the same as tech-report, decide whether to reword

While heterogeneous systems are becoming more popular, their
programming models deter many potential users.  Unlike adding more or
faster CPUs, where existing programming models work without code changes,
%which work without code changes, 
programs must be
explicitly updated to use GPUs and other accelerators. Rather than
grapple with unfamiliar programming models, users often run their
CPU-only code on accelerated resources, leaving a significant portion
of the computing resources idle.  Accelerated
OpenMP, our term for a class of
directive-based programming models including OpenMP for
Accelerators~\cite{Beyer:2011ib} and the PGI accelerator
model~\cite{Wolfe:2010bk} among others, can ease this transition by
allowing users to target accelerators with a familiar OpenMP-style
syntax.
% This syntax will facilitate the adoption of accelerators in scientific
% computing, especially for legacy OpenMP applications.
However, Accelerated OpenMP is not a panacea: current iterations help one
\emph{move} their computation to a \emph{single} accelerator with
straightforward syntax. Once
%so
moved however, there is no way to workshare a
loop across multiple devices without \emph{manually} targeting each device.

In order to target, for example, a GPU and four CPU cores, a user must
manually split the work, run that work on each separate device, and
manually merge each result.  Any load balancing, coherency, or runtime
adaptation of any kind must be reimplemented by every user.  So,
while Accelerated OpenMP can parallelize serial code via annotation, it
lacks the ability to scale and to load-balance work transparently on the hardware
found at runtime.

Our work
%seeks to allow
enables safe and efficient worksharing across devices in
Accelerated OpenMP.
%We must address two primary concerns to offer such a
%construct.
To do so, we must address two primary concerns. First, we
%must
manage memory input and output across multiple address spaces without
requiring alterations to the associated parallel loop.  Second, we
%must
divide work across devices with vastly different computational
capabilities fairly and efficiently.  In all, our \tsarlong library
automates the scheduling, load balancing, and cross-device data
management for safe and efficient
%that is necessary to efficiently implement
%this
worksharing.
% construct.
This paper presents the design and implementation of \tsar and the
extended Accelerated OpenMP syntax to integrate its functionality.
Specifically, we make the following contributions:

\begin{itemize}

    \item The design and syntax of a multi-target, worksharing
      construct for Accelerated OpenMP;

    \item The design, implementation, and optimization of our
      scheduling and memory management library, \tsar, which can be
      used with any Accelerated OpenMP compiler/runtime or with CUDA
      and CPU OpenMP directly;

    \item Seven adaptive scheduling policies, spanning from
      a low-overhead but
      coarse-grained adaptive approach
      to a chunk-based, fine-grained scheduling
      approach for distributing work.

    \item A rigorous performance evaluation of \tsar
      that demonstrates how runtime scheduling can significantly
      improve performance while maintaining programmability.

\end{itemize}

% Our comparison to a state of the art heterogeneous task scheduling library
% demonstrates the utility of \tsar's ability to adapt task granularity
% dynamically. Overall, \tsar achieves quantitative benefits of up to 20\% over
% static coscheduling, and more than 50\% over OmpSs and StarPU.

The rest of the paper is composed as
follows. Section~\ref{sec:background} offers motivation and background.
Section~\ref{sec:design} describes the design of \tsar,
including our task management concept, scheduling mechanisms, and
memory management. Details on our implementation follow in
Section~\ref{sec:imp}.  Section~\ref{sec:results} present our results.
Related work follows in Section~\ref{sec:related} and conclusions in
Section~\ref{sec:conclusion}.

