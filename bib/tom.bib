
@article{Chen:2010cy,
  author = {Chen, Long and Villa, O and Krishnamoorthy, S and Gao, G.R},
  title = {{Dynamic Load Balancing on Single- and Multi-GPU Systems}},
  journal = {2010 IEEE International Parallel {\&} Distributed Processing Symposium (IPDPS)},
  year = {2010},
  pages = {1--12}
}

@inproceedings{Rountree:2009in,
  author = {Rountree, Barry and Lownenthal, David K and de Supinski, Bronis R. and Schulz, Martin and Freeh, Vincent W and Bletsch, Tyler},
  title = {{Adagio: Making DVS Practical for Complex HPC Applications}},
  booktitle = {ICS '09: Proceedings of the 23rd International Conference on Supercomputing},
  year = {2009},
  publisher = { ACM},
  month = jun
}

@inproceedings{Ayguade:2003tw,
  author = {Ayguad{\'e}, Eduard and Blainey, Bob and Duran, Alejandro and Labarta, Jes{\'u}s and Mart{\'\i}nez, Francisco and Martorell, Xavier and Silvera, Ra{\'u}l},
  title = {{Is the Schedule Clause Really Necessary in OpenMP?}},
  booktitle = {WOMPAT'03: Proceedings of the Workshop on OpenMP Applications and Tools 2003},
  year = {2003},
  publisher = { Springer-Verlag},
  month = jun
}

@article{Bueno:2012ka,
  author = {Bueno, J and Planas, J. and Duran, A. and Badia, R.M. and Martorell, X. and Ayguade, E. and Labarta, J.},
  title = {{Productive Programming of GPU Clusters with OmpSs}},
  journal = {26th IEEE Int'l Parallel {\&} Distributed Processing Symp. (IPDPS)},
  year = {2012},
  pages = {557--568}
}


@article{khronos2008opencl,
  title={{The OpenCL Specification}},
  author={{Khronos OpenCL Working Group and others}},
  journal={A. Munshi, Ed.},
  year={2008}
}
@misc{nvidia2008programming,
  title={{NVIDIA CUDA Programming Guide}},
  author={},
  year={2008}
}
@article{DURAN:2011uy,
  author = {Duran, A. and Ayguade, E. and Badia, R.M. and Labarta, J. and Martinell, L. and Martorell, X. and Planas, J.},
  title = {{OmpSs: A Proposal for Programming Heterogeneous Multi-Core Architectures}},
  journal = {Parallel Processing Letters},
  year = {2011},
  volume = {21},
  number = {2},
  pages = {173--193}
}


@inproceedings{he2008mars,
  author = {He, B and Fang, W and Luo, Q and Govindaraju, N K and Wang, T},
  title = {{Mars: a MapReduce framework on graphics processors}},
  booktitle = {Proceedings of the 17th international conference on Parallel architectures and compilation techniques},
  year = {2008},
  pages = {260--269},
  organization = {ACM New York, NY, USA}
}

@inproceedings{Hong:2010kj,
  author = {Hong, Chuntao and Chen, Dehao and Chen, Wenguang and Zheng, Weimin and Lin, Haibo},
  title = {{MapCG: writing parallel program portable between CPU and GPU}},
  booktitle = {PACT '10: Proceedings of the 19th international conference on Parallel architectures and compilation techniques},
  year = {2010},
  publisher = { ACM  Request Permissions},
  month = sep
}
@inproceedings{Elteir:2011ec,
  author = {Elteir, M and Lin, Heshan and Feng, Wu-chun and Scogland, T},
  title = {{StreamMR: An Optimized MapReduce Framework for AMD GPUs}},
  booktitle = {Parallel and Distributed Systems (ICPADS), 2011 IEEE 17th International Conference on},
  year = {2011},
  pages = {364--371},
  publisher = { IEEE Computer Society}
}

@inproceedings{Ravi:2010hw,
  author = {Ravi, Vignesh T and Ma, Wenjing and Chiu, David and Agrawal, Gagan},
  title = {{Compiler and runtime support for enabling generalized reduction computations on heterogeneous parallel configurations}},
  booktitle = {ICS '10: Proceedings of the 24th ACM International Conference on Supercomputing},
  year = {2010},
  publisher = { ACM  Request Permissions},
  month = jun
}

@inproceedings{daga2011architecture,
  title={{Architecture-Aware Mapping and Optimization on a 1600-Core GPU}},
  author={Daga, M. and Scogland, T. and Feng, W.},
  booktitle={17th IEEE Int'l Conf. on Parallel and Distributed Systems},
  pages={316--323},
  year={2011},
  organization={IEEE}
}


@inproceedings{Ravi:2011ie,
  author = {Ravi, V T and Agrawal, G},
  title = {{A Dynamic Scheduling Framework for Emerging Heterogeneous Systems}},
  booktitle = {18th International Conference on High Performance Computing (HiPC)},
  year = {2011},
  pages = {1--10}
}

@article{reinders_intel_2007,
  title = {{Intel Threading Building Blocks}},
  author = {Reinders, J.},
  year = {2007}
},

  @inproceedings{lee_openmp_2008,
    title = {{OpenMP} to {GPGPU}},
    isbn = {9781605583976},
    doi = {10.1145/1504176.1504194},
    booktitle = {{PPoPP} '09 Proceedings of the 14th {ACM} {SIGPLAN} symposium on Principles and practice of parallel programming},
    publisher = {{ACM} Press},
    author = {Lee, Seyong and Min, {Seung-Jai} and Eigenmann, Rudolf},
    year = {2008},
    keywords = {{IPDPS12}},
    pages = {101}
  },

  @inproceedings{rafique_cellmr:_2009,
    title = {{{CellMR:} A Framework for Supporting MapReduce on Asymmetric Cell-Based Clusters}},
    isbn = {978-1-4244-3751-1},
    shorttitle = {{CellMR}},
    doi = {10.1109/IPDPS.2009.5161062},
    abstract = {The use of asymmetric multi-core processors with on-chip computational accelerators is becoming common in a variety of environments ranging from scientific computing to enterprise applications. The focus of current research has been on making efficient use of individual systems, and porting applications to asymmetric processors. In this paper, we take the next step by investigating the use of multi-core-based systems, especially the popular Cell processor, in a cluster setting. We present {CellMR}, an efficient and scalable implementation of the {MapReduce} framework for asymmetric Cell-based clusters. The novelty of {CellMR} lies in its adoption of a streaming approach to supporting {MapReduce}, and its adaptive resource scheduling schemes: Instead of allocating workloads to the components once, {CellMR} slices the input into small work units and streams them to the asymmetric nodes for efficient processing. Moreover, {CellMR} removes {I/O} bottlenecks by design, using a number of techniques, such as double-buffering and asynchronous {I/O}, to maximize cluster performance. Our evaluation of {CellMR} using typical {MapReduce} applications shows that it achieves 50.5\% better performance compared to the standard nonstreaming approach, introduces a very small overhead on the manager irrespective of application input size, scales almost linearly with increasing number of compute nodes (a speedup of 6.9 on average, when using eight nodes compared to a single node), and adapts effectively the parameters of its resource management policy between applications with varying computation density.},
    booktitle = {{IEEE} International Symposium on Parallel \& Distributed Processing, 2009. {IPDPS} 2009},
    publisher = {{IEEE}},
    author = {Rafique, M. M and Rose, B. and Butt, A. R and Nikolopoulos, D. S},
    month = may,
    year = {2009},
    keywords = {Acceleration, adaptive resource scheduling, asymmetric Cell-based cluster, asymmetric multicore processor, asynchronous {I/O}, Cell processor, {CellMR}, Computer architecture, Data processing, Distributed computing, double buffering, enterprise application, environment ranging, High performance computing, {IPDPS12}, {MapReduce}, microprocessor chips, Multicore processing, multiprocessing systems, on-chip computational accelerator, Parallel architectures, Parallel programming, Processor scheduling, resource allocation, Resource management, scheduling, scientific computing, workstation clusters},
    pages = {1--12}
  },

  @incollection{seznec_predictive_????,
    address = {Berlin, Heidelberg},
    title = {{Predictive Runtime Code Scheduling for Heterogeneous Architectures}},
    volume = {5409},
    isbn = {978-3-540-92989-5, 978-3-540-92990-1},
    booktitle = {High Performance Embedded Architectures and Compilers},
    publisher = {Springer},
    author = {Jim\'enez, Víctor J. and Vilanova, Lluís and Gelado, Isaac and Gil, Marisa and Fursin, Grigori and Navarro, Nacho},
    editor = {Seznec, André and Emer, Joel and {O’Boyle}, Michael and Martonosi, Margaret and Ungerer, Theo},
    pages = {19--33}
  },

  @article{fenley_analytical_2008,
    title = {{An Analytical Approach to Computing Biomolecular Electrostatic Potential. I. Derivation and Analysis}},
    volume = {129},
    issn = {00219606},
    doi = {10.1063/1.2956497},
    journal = {The Journal of Chemical Physics},
    author = {Fenley, Andrew T. and Gordon, John C. and Onufriev, Alexey},
    year = {2008}
  },

  @incollection{sips_starpu:_2009,
    address = {Berlin, Heidelberg},
    title = {{{StarPU:} A Unified Platform for Task Scheduling on Heterogeneous Multicore Architectures}},
    volume = {5704},
    isbn = {978-3-642-03868-6, 978-3-642-03869-3},
    shorttitle = {{StarPU}},
    booktitle = {{Euro-Par} 2009 Parallel Processing},
    publisher = {Springer},
    author = {Augonnet, Cédric and Thibault, Samuel and Namyst, Raymond and Wacrenier, {Pierre-André}},
    editor = {Sips, Henk and Epema, Dick and Lin, {Hai-Xiang}},
    year = {2009},
    keywords = {{IPDPS12}},
    pages = {863--874}
  },

  @inproceedings{basumallik_programming_2007,
    title = {Programming Distributed Memory Sytems Using {OpenMP}},
    isbn = {1-4244-0910-1},
    doi = {10.1109/IPDPS.2007.370397},
    abstract = {{OpenMP} has emerged as an important model and language extension for shared-memory parallel programming. On shared-memory platforms, {OpenMP} offers an intuitive, incremental approach to parallel programming. In this paper, we present techniques that extend the ease of shared-memory parallel programming in {OpenMP} to distributed-memory platforms as well. First, we describe a combined compile-time/runtime system that uses an underlying software distributed shared memory system and exploits repetitive data access behavior in both regular and irregular program sections. We present a compiler algorithm to detect such repetitive data references and an {API} to an underlying software distributed shared memory system to orchestrate the learning and proactive reuse of communication patterns. Second, we introduce a direct translation of standard {OpenMP} into {MPI} message-passing programs for execution on distributed memory systems. We present key concepts and describe techniques to analyze and efficiently handle both regular and irregular accesses to shared data. Finally, we evaluate the performance achieved by our approaches on representative {OpenMP} applications.},
    booktitle = {Parallel and Distributed Processing Symposium, 2007. {IPDPS} 2007. {IEEE} International},
    publisher = {{IEEE}},
    author = {Basumallik, A. and Min, S. {-J} and Eigenmann, R.},
    month = mar,
    year = {2007},
    keywords = {{API}, application program interface, application program interfaces, Application software, Communication system software, compiler algorithm, Concurrent computing, data access behavior, Distributed computing, distributed shared memory systems, {IPDPS12}, Message passing, message-passing program, {MPI}, open systems, {OpenMP}, Parallel programming, program compilers, Runtime, Software algorithms, software distributed shared-memory parallel program, Software performance, Software systems},
    pages = {1--8}
  },

  @inproceedings{Bueno:2013jd,
    author = {J. Bueno and X. Martorell and R. Badia and E. Aygaude and J. Labarta}, 
   title = {{Implementing OmpSs Support for Regions of Data in Architectures with Multiple Address Spaces}},
    booktitle = {International Conference on Supercomputing ICS'13},
    year = {2013},
    pages = {359--368},
    publisher = {ACM Press},
    address = {New York, New York, USA},
    doi = {10.1145/2464996.2465017},
    isbn = {9781450321303},
    read = {Yes},
    rating = {0},
    date-added = {2014-02-21T23:58:31GMT},
    date-modified = {2014-03-18T14:11:09GMT},
    url = {http://dl.acm.org/citation.cfm?doid=2464996.2465017},
    local-url = {file://localhost/Users/njustn/Dropbox/Papers2/Articles/2013/Unknown/Implementing_OmpSs_support_for_regions_of_data_in_architectures_with_multiple_address_spaces_2013.pdf},
    file = {{Implementing_OmpSs_support_for_regions_of_data_in_architectures_with_multiple_address_spaces_2013.pdf:/Users/njustn/Dropbox/Papers2/Articles/2013/Unknown/Implementing_OmpSs_support_for_regions_of_data_in_architectures_with_multiple_address_spaces_2013.pdf:application/pdf}},
    uri = {\url{papers2://publication/doi/10.1145/2464996.2465017}}
  }

@article{anandakrishnan_accelerating_2010,
  title = {{Accelerating Electrostatic Surface Potential Calculation with Multi-Scale Approximation on Graphics Processing Units}},
  volume = {28},
  issn = {1093-3263},
  doi = {10.1016/j.jmgm.2010.04.001},
  abstract = {Tools that compute and visualize biomolecular electrostatic surface potential have been used extensively for studying biomolecular function. However, determining the surface potential for large biomolecules on a typical desktop computer can take days or longer using currently available tools and methods. Two commonly used techniques to speed-up these types of electrostatic computations are approximations based on multi-scale coarse-graining and parallelization across multiple processors. This paper demonstrates that for the computation of electrostatic surface potential, these two techniques can be combined to deliver significantly greater speed-up than either one separately, something that is in general not always possible. Specifically, the electrostatic potential computation, using an analytical linearized {Poisson–Boltzmann} {(ALPB)} method, is approximated using the hierarchical charge partitioning {(HCP)} multi-scale method, and parallelized on an {ATI} Radeon 4870 graphical processing unit {(GPU).} The implementation delivers a combined 934-fold speed-up for a 476,040 atom viral capsid, compared to an equivalent non-parallel implementation on an Intel E6550 {CPU} without the approximation. This speed-up is significantly greater than the 42-fold speed-up for the {HCP} approximation alone or the 182-fold speed-up for the {GPU} alone.},
  number = {8},
  journal = {Journal of Molecular Graphics and Modelling},
  author = {Anandakrishnan, Ramu and Scogland, Tom {R.W.} and Fenley, Andrew T. and Gordon, John C. and Feng, Wu-chun and Onufriev, Alexey V.},
  keywords = {Biomolecular electrostatics, Graphical processing unit {(GPU)}, Implicit solvent model, Multi-scale modeling},
  year = {2009},
  pages = {904--910}
},

  @techreport{OpenMP40tr1,
    title = {{OpenMP 4.0 Public Review Release Candidate 1}},
    howpublished= {http://www.openmp.org/mp-documents/OpenMP4.0RC1_final.pdf},
  }


@inproceedings{scogland:7Hpt64iV,
  author = {Scogland, Thomas R W and Rountree, Barry and Feng, W and de Supinski, Bronis R},
  title = {{CoreTSAR: Adaptive Worksharing for Heterogeneous Systems}},
  booktitle = {International Supercomputing Conference (ISC'14)},
  year = {2014},
  rating = {0},
  date-added = {2013-11-28T17:47:38GMT},
  date-modified = {2014-03-14T16:21:46GMT},
  uri = {\url{papers2://publication/uuid/EC7A6DEB-8895-4531-A7F5-7DE664595DD5}}
}

@techreport{Scogland:wi,
  author = {Scogland, TRW and Feng, W and Rountree, B and de Supinski, B R},
  title = {{CoreTSAR: Task Scheduling for Accelerator-aware Runtimes}},
  institution = {Computer Science Technical Report TR-12-20, Virginia Tech},
  address = {Blacksburg, Virginia, USA},
  year = {2012}
}



@inproceedings{Scogland:wt,
  author = {Scogland, Thomas R. W. and Rountree, Barry and Feng, Wu and de Supinski, Bronis R.},
  title = {{Heterogeneous Task Scheduling for Accelerated OpenMP}},
  booktitle = {IEEE Int'l Parallel {\&} Distributed Processing Symp. (IPDPS)},
  address = {Shanghai, China}, 
  month = {May}, 
  year = {2012}
}



@article{gordon_analytical_2008,
  title = {{An Analytical Approach to Computing Biomolecular Electrostatic Potential. {II.} Validation and Applications}},
  volume = {129},
  issn = {00219606},
  doi = {10.1063/1.2956499},
  journal = {The Journal of Chemical Physics},
  author = {Gordon, John C. and Fenley, Andrew T. and Onufriev, Alexey},
  year = {2008},
  pages = {075102}
},

  @techreport{dua_cuda_????,
    title = {{From {CUDA} to {OpenCL:} Towards a Performance-Portable Solution for Multi-Platform {GPU} Programming}},
    shorttitle = {From {CUDA} to {OpenCL}},
    institution = {Technical Report {CS-10-656}, Electrical Engineering and Computer Science Department, University of Tennessee},
    author = {Dua, P. and Webera, R. and Luszczeka, P. and Tomova, S. and Petersona, G. and Dongarraa, J.}
  },

  @article{dagum_openmp:_1998,
    title = {{{OpenMP:} An Industry Standard {API} for Shared-Memory Programming}},
    volume = {5},
    issn = {1070-9924},
    shorttitle = {{OpenMP}},
    doi = {10.1109/99.660313},
    number = {1},
    journal = {{IEEE} Computational Science \& Engineering},
    author = {Dagum, L. and Menon, R.},
    month = mar,
    year = {1998},
    keywords = {allocatables, {ANSI} standards, application program interfaces, callable runtime library, callable runtime library routines, coarse grain parallelism, Coherence, compiler directives, Computer architecture, environment variables, Fortran, Fortran 90, Fortran 95, Fortran compiler, Hardware, industry standard {API}, {IPDPS12}, Message passing, {OpenMP}, Parallel processing, Parallel programming, pointers, Power system modeling, Scalability, shared memory parallelism, shared memory programming, shared memory systems, software portability, software reviews, software standards, Software systems, {X3H5} concepts},
    pages = {46--55}
  },

  @article{lin_reliable_2011,
    title = {{Reliable {MapReduce} Computing on Opportunistic Resources}},
    issn = {1386-7857, 1573-7543},
    doi = {10.1007/s10586-011-0158-7},
    journal = {Cluster Computing},
    author = {Lin, Heshan and Ma, Xiaosong and Feng, Wu-chun},
    month = feb,
    year = {2011}
  },

  @article{mccormick_scout:_2007,
    title = {{Scout: A Data-Parallel Programming Language for Graphics Processors}},
    issn = {01678191},
    shorttitle = {Scout},
    doi = {10.1016/j.parco.2007.09.001},
    journal = {Parallel Computing},
    author = {McCormick, P and Inman, J and Ahrens, J and Mohdyusof, J and Roth, G and Cummins, S},
    month = sep,
    year = {2007},
    keywords = {{IPDPS12}}
  },


  @incollection{chapman_openmp_2011,
    address = {Berlin, Heidelberg},
    title = {{{OpenMP} for Accelerators}},
    volume = {6665},
    isbn = {978-3-642-21486-8, 978-3-642-21487-5},
    booktitle = {{OpenMP} in the Petascale Era},
    publisher = {Springer},
    author = {Beyer, James C. and Stotzer, Eric J. and Hart, Alistair and de Supinski, Bronis R.},
    editor = {Chapman, Barbara M. and Gropp, William D. and Kumaran, Kalyan and Müller, Matthias S.},
    year = {2011},
    keywords = {{IPDPS12}},
    pages = {108--121}
  },

  @inproceedings{kale_charm++_1993,
    title = {{CHARM++}},
    isbn = {0897915879},
    doi = {10.1145/165854.165874},
    booktitle = {{OOPSLA} '93 Proceedings of the Eighth Annual Conference on Object-Oriented Programming Systems, Languages, and Applications},
    publisher = {{ACM} Press},
    author = {Kale, Laxmikant V. and Krishnan, Sanjeev},
    year = {1993},
    keywords = {{IPDPS12}},
    pages = {91--108}
  },

  @article{dean_mapreduce_2008,
    title = {{MapReduce}},
    volume = {51},
    issn = {00010782},
    doi = {10.1145/1327452.1327492},
    journal = {Communications of the {ACM}},
    author = {Dean, Jeffrey and Ghemawat, Sanjay},
    month = jan,
    year = {2008},
    pages = {107}
  },

  @inproceedings{archuleta_multi-dimensional_2009,
    address = {Los Alamitos, {CA}, {USA}},
    title = {{Multi-Dimensional Characterization of Temporal Data Mining on Graphics Processors}},
    isbn = {978-1-4244-3751-1},
    doi = {http://doi.ieeecomputersociety.org/10.1109/IPDPS.2009.5161049},
    abstract = {Through the algorithmic design patterns of data parallelism and task parallelism, the graphics processing unit {(GPU)} offers the potential to vastly accelerate discovery and innovation across a multitude of disciplines. For example, the exponential growth in data volume now presents an obstacle for high-throughput data mining in fields such as neuroscience and bioinformatics. As such, we present a characterization of a {MapReduced-based} data-mining application on a general-purpose {GPU} {(GPGPU).} Using neuroscience as the application vehicle, the results of our multi-dimensional performance evaluation show that a “one-size-fits-all” approach maps poorly across different {GPGPU} cards. Rather, a high-performance implementation on the {GPGPU} should factor in the 1) problem size, 2) type of {GPU}, 3) type of algorithm, and 4) data-access method when determining the type and level of parallelism. To guide the {GPGPU} programmer towards optimal performance within such a broad design space, we provide eight general performance characterizations of our data-mining application.},
    booktitle = {Parallel and Distributed Processing Symposium, International},
    publisher = {{IEEE} Computer Society},
    author = {Archuleta, Jeremy and Cao, Yong and Scogland, Tom and Feng, Wu-chun},
    year = {2009},
    pages = {1--12},
    annote = {Complete {PDF} document was either not available or accessible. Please make sure you're logged in to the digital library to retrieve the complete {PDF} document.}
  },

  @article{bailey_nas_1991,
    title = {{The NAS Parallel Benchmarks}},
    volume = {5},
    doi = {10.1177/109434209100500306},
    abstract = {A new set of benchmarks has been developed for the performance evaluation of highly parallel supercom puters. These consist of five "parallel kernel" bench marks and three "simulated application" benchmarks. Together they mimic the computation and data move ment characteristics of large-scale computational fluid dynamics applications. The principal distinguishing feature of these benchmarks is their "pencil and paper" specification—all details of these benchmarks are specified only algorithmically. In this way many of the difficulties associated with conventional bench- marking approaches on highly parallel systems are avoided.},
    number = {3},
    journal = {International Journal of High Performance Computing Applications},
    author = {Bailey, {D.H.} and Barszcz, E. and Barton, {J.T.} and Browning, {D.S.} and Carter, {R.L.} and Dagum, L. and Fatoohi, {R.A.} and Frederickson, {P.O.} and Lasinski, {T.A.} and Schreiber, {R.S.} and Simon, {H.D.} and Venkatakrishnan, V. and Weeratunga, {S.K.}},
    year = {1991},
    pages = {63 --73}
  },

  @inproceedings{luk_qilin_2009,
    title = {{Qilin: Exploiting Parallelism on Heterogeneous Multiprocessors with Adaptive Mapping}},
    booktitle = {Proc. 42nd IEEE/ACM International Symposium on Microarchitecture},
    isbn = {9781605587981},
    doi = {10.1145/1669112.1669121},
    publisher = {{ACM} Press},
    author = {Luk, {Chi-Keung} and Hong, Sunpyo and Kim, Hyesoon},
    year = {2009},
  }

@inproceedings{dolbeau2007hmpp,
  title={{HMPP:} A Hybrid Multi-Core Parallel Programming Environment},
  author={Dolbeau, R. and Bihan, S. and Bodin, F.},
  booktitle={Workshop on General Purpose Processing on Graphics Processing Units},
  year={2007}
}

@misc{berkelaar2003lp_solve,
  title={lp\_solve:(Mixed integer) linear programming problem solver},
  author={Berkelaar, M. and Notebaert, P. and Eikland, K.},
  howpublished= {http://lpsolve.sourceforge.net/5.0/},
  year={2003}
}

@misc{openacc,
  title={{The OpenACC Application Programming Interface, v1.0}},
  author={{CAPS Enterprise, Cray Inc., NVIDIA and the Portland Group}},
  howpublished={http://www.openacc-standard.org},
  month={Nov.},
  year={2011}
}

